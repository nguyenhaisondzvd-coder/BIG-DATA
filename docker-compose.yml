version: '3.8'

services:
  # Hadoop NameNode
  namenode:
    build: .
    container_name: namenode
    hostname: namenode
    ports:
      - "9870:9870"  # HDFS NameNode UI
      - "9000:9000"  # HDFS NameNode
    environment:
      - CLUSTER_NAME=bigdata-cluster
      - HDFS_NAMENODE_USER=root
      - HDFS_DATANODE_USER=root
      - HDFS_SECONDARYNAMENODE_USER=root
      - YARN_RESOURCEMANAGER_USER=root
      - YARN_NODEMANAGER_USER=root
    volumes:
      - hadoop_namenode:/opt/hadoop/dfs/name
      - ./data:/app/data
      - ./models:/app/models
      - ./results:/app/results
      - ./config/hadoop/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml
      - ./config/hadoop/hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml
    networks:
      - bigdata-network
    command: >
      /bin/bash -c "
        if [ ! -d /opt/hadoop/dfs/name/current ]; then
          /opt/hadoop/bin/hdfs namenode -format -force;
        fi &&
        /opt/hadoop/sbin/hadoop-daemon.sh start namenode &&
        tail -f /opt/hadoop/logs/hadoop-root-namenode-*.log
      "

  # Hadoop DataNode 1
  datanode:
    build: .
    container_name: datanode
    hostname: datanode
    ports:
      - "9864:9864"  # HDFS DataNode UI
    environment:
      - CLUSTER_NAME=bigdata-cluster
      - HDFS_NAMENODE_USER=root
      - HDFS_DATANODE_USER=root
    volumes:
      - hadoop_datanode1:/opt/hadoop/dfs/data
      - ./data:/app/data
      - ./config/hadoop/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml
      - ./config/hadoop/hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml
    networks:
      - bigdata-network
    depends_on:
      - namenode
    command: >
      /bin/bash -c "
        /opt/hadoop/sbin/hadoop-daemon.sh start datanode &&
        tail -f /opt/hadoop/logs/hadoop-root-datanode-*.log
      "

  # Hadoop DataNode 2
  datanode2:
    build: .
    container_name: datanode2
    hostname: datanode2
    ports:
      - "9865:9864"  # HDFS DataNode UI
    environment:
      - CLUSTER_NAME=bigdata-cluster
      - HDFS_NAMENODE_USER=root
      - HDFS_DATANODE_USER=root
    volumes:
      - hadoop_datanode2:/opt/hadoop/dfs/data
      - ./data:/app/data
      - ./config/hadoop/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml
      - ./config/hadoop/hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml
    networks:
      - bigdata-network
    depends_on:
      - namenode
    command: >
      /bin/bash -c "
        /opt/hadoop/sbin/hadoop-daemon.sh start datanode &&
        tail -f /opt/hadoop/logs/hadoop-root-datanode-*.log
      "

  # Spark Master
  spark-master:
    build: .
    container_name: spark-master
    hostname: spark-master
    ports:
      - "8080:8080"  # Spark Master UI
      - "7077:7077"  # Spark Master
      - "4040:4040"  # Spark App UI
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8080
      - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
      - SPARK_NO_DAEMONIZE=1
    volumes:
      - ./data:/app/data
      - ./models:/app/models
      - ./results:/app/results
      - ./config/hadoop/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml
      - ./config/hadoop/hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml
      - ./config/spark/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
    networks:
      - bigdata-network
    depends_on:
      - namenode
    command: >
      /bin/bash -c "
        /opt/spark/sbin/start-master.sh &&
        sleep 5 &&
        tail -f /opt/spark/logs/*.out
      "

  # Spark Worker 1
  spark-worker-1:
    build: .
    container_name: spark-worker-1
    hostname: spark-worker-1
    ports:
      - "8081:8081"  # Spark Worker UI
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
      - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
      - SPARK_NO_DAEMONIZE=1
    volumes:
      - ./data:/app/data
      - ./models:/app/models
      - ./results:/app/results
      - ./config/hadoop/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml
      - ./config/hadoop/hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml
    networks:
      - bigdata-network
    depends_on:
      - spark-master
    command: >
      /bin/bash -c "
        sleep 10 &&
        /opt/spark/sbin/start-worker.sh spark://spark-master:7077 &&
        sleep 5 &&
        tail -f /opt/spark/logs/*.out
      "

  # Spark Worker 2
  spark-worker-2:
    build: .
    container_name: spark-worker-2
    hostname: spark-worker-2
    ports:
      - "8082:8081"  # Spark Worker UI
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
      - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
      - SPARK_NO_DAEMONIZE=1
    volumes:
      - ./data:/app/data
      - ./models:/app/models
      - ./results:/app/results
      - ./config/hadoop/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml
      - ./config/hadoop/hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml
    networks:
      - bigdata-network
    depends_on:
      - spark-master
    command: >
      /bin/bash -c "
        sleep 15 &&
        /opt/spark/sbin/start-worker.sh spark://spark-master:7077 &&
        sleep 5 &&
        tail -f /opt/spark/logs/*.out
      "

  # Application Runner
  app-runner:
    build: .
    container_name: app-runner
    hostname: app-runner
    environment:
      - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
      - SPARK_MASTER=spark://spark-master:7077
      - HDFS_NAMENODE_USER=root
      - HDFS_DATANODE_USER=root
    volumes:
      - ./data:/app/data
      - ./models:/app/models
      - ./results:/app/results
      - ./config/hadoop/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml
      - ./config/hadoop/hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml
      - ./config/spark/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf

    networks:
      - bigdata-network
    depends_on:
      - spark-master
      - spark-worker-1
      - spark-worker-2
    command: >
      /bin/bash -c "
        echo 'Waiting for HDFS to be ready...' &&
        until /opt/hadoop/bin/hdfs dfsadmin -report >/dev/null 2>&1; do sleep 5; done &&
        echo '✅ HDFS is ready!' &&

        echo 'Ensuring HDFS directories exist...' &&
        /opt/hadoop/bin/hdfs dfs -mkdir -p /spark-logs /user/recommendation/data /user/recommendation/models /user/recommendation/results &&
        /opt/hadoop/bin/hdfs dfs -chmod -R 777 /spark-logs /user/recommendation &&
        echo '✅ HDFS directories created!' &&

        echo 'Waiting for Spark Master...' &&
        until nc -z spark-master 7077; do sleep 5; done &&
        echo '✅ Spark Master is ready!' &&

        echo 'Starting recommendation system...' &&
        python main.py
      "

networks:
  bigdata-network:
    driver: bridge

volumes:
  hadoop_namenode:
  hadoop_datanode1:
  hadoop_datanode2: